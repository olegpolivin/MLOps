# Review of papers

1. [Hidden Technical Debt in Machine Learning Systems Review](#hiddendebt)
2. [MLOps: Continuous delivery and automation pipelines in machine learning Review](#MLOPSCI/CD)
3. [Rules of Machine Learning: Best Practices for ML Engineering Review](#RulesOfML)

# Week of 19-Apr-2021

<a name="hiddendebt"></a>
## [Hidden Technical Debt in Machine Learning Systems](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)

Machine Learning systems have a special capacity to generate technical debt. A trap here is that one can get an ML model fast, but the problem is that one pays for it by accumulating technical debt. Here are different facets of technical debt generated by ML systems:

### 1) You want your code to be modular - ML development erodes this modularity
How does this happen?
- Entanglement -> Inputs (think of features) are not independent of each other. Moreover, CACE principle (Changing Anything Changes Everything) applies. Imagine you change features that you use. Then quite probably you have to change hyper-parameters, learning settings, thresholds and other things. 
- Correction Cascades -> The way I got it: you learn one model M to solve a particular problem. And then you use, say, the outputs of model M, to solve another problem, the one you are, actually, interested in. Imagine a cascade of such models. In that case a modification in one place, will need more modifications down the line. It is quite costly, and thus might impede improvements: better not touch something that is so complex.
- Undeclared consumers -> Someone uses your model for their own needs. Then if what they are doing is important, your improvements or modifications could be difficult to perform, because other divisions/people depend on your code.

### 2) Data Dependency
What is it about?
- Unstable data dependencies -> Data changes. You model can have unpredicted behaviour due to the changing nature of the data.
- Underutilized Data Dependencies. -> You accumulate features. Imagine a feature takes a very long time to calculate but is/becomes redundant. You keep calculating it, but you don't have time to get rid of it. Underutilized dependencies can be detected via exhaustive leave-one-feature-out evaluations. These should be run regularly to identify and remove unnecessary features.
- Static Analysis of Data Dependencies -> The idea is to run automatic chcecks to ensure that all dependencies have the appropriate annotations. Needs further investigation.

### 3) Feedback Loops
That's, actually, nice. I guess it is quite difficult to detect. But it is the case when live ML model ends up influencing its own behaviour. A more involved case when you have multiple disjoint ML systems that influence one another. Example from the paper:
> Consider the case of two stock-market prediction models from two different investment companies. Improvemets (or, more scarily, bugs) in one may influence the bidding and buying behaviour of the other.

### 4) ML-System Anti-Patterns

- Glue code -> When using a generic package, you might need to write your code in such a way so that it can be used in that side package. It might lead to a massive ammount of supportive code to be written just to get data in and out of this generic package. Sometimes, it could be better to write your own simpler code. Another way to deal with glue code is to wrap such package into a common API that is easy to re-use.
- Pipeline jungles -> creating one pipeline to treat data, then another, then another - only to end up with lots of pipelines. 
- Data Experimental Codepaths -> Develop your experiments, forget about them, and then not clear where to look.
- Abstraction Debt -> Further investigation needed.
- Common Smells -> What might indicate a problem:
  - Plain-Old-Data Type Smell -> you need to know what a parameter represents. Is it a log-odds multiplier or a decision threshold?
  - Multiple-Language Smell -> Combination of languages is not a good thing.
  - Prototype Smell -> Having a separate prototyping environment is not good, because those tests might not correspond to reality at full scale. 

### 5) Configuration Debt

Configuration of configurations might become too difficult to manage. Some suggestions:
- It should be easy to specify a configuration as a small change from a previous configuration.
- It should be hard to make manual errors, omissions, or oversights.
- It should be easy to see, visually, the difference in configuration between two models.
- It should be easy to automatically assert and verify basic facts about the configuration: number of features used...
- It should be possible to detect unused or redundant settings
- Configurations should undergo a full code review and be checked into a repository.

### 6) Dealing with Changes in the External World

- Fixed Thresholds in Dynamic Systems -> Imagine new data arrives, but your threshold is hardcoded. Not a good solution.
- Monitoring and Testing -> Test, test, test. Although it might be difficult to understand what to test. Well
  - usual unit tests
  - distribution of predicted labels is equal to the distribution of observed labels
  - if your system might take actions - put limits on those actions
  - Monitor and test what's coming from upstream sources.

### 7) Other Areas of ML-related Debt

- Data Testing Debt -> The idea looks similar to [great expectations](https://greatexpectations.io) package in Python. Basically: 'test data'.
- Reproducibility Debt -> Are your ML predictions reproducible?
- Process Management Debt -> Imagine all the problems above, but for a huge number of models.
- Cultural Debt -> Let ML research and Engineering communicate. Probably, it should be the same person.

### Suggestions
How can we measure debt? Try to answer the following questions:

 - How easily can an entirely new algorithmic approach be tested at full scale? 
 - What is the transitive closure of all data dependencies?
 - How precisely can the impact of a new change to the system be measured?
 - Does improving one model or signal degrade others?
 - How quickly can new members of the team be brought up to speed?



<a name="MLOPSCI/CD"></a>
## [MLOps: Continuous delivery and automation pipelines in machine learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)

<a name="RulesOfML"></a>
## [Rules of Machine Learning: Best Practices for ML Engineering](https://developers.google.com/machine-learning/guides/rules-of-ml)
